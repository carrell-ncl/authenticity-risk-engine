{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea8870d",
   "metadata": {},
   "source": [
    "# Calibrator Debug Notebook\n",
    "\n",
    "This notebook helps you debug why the **LR calibrator** is producing unexpectedly low probabilities.\n",
    "\n",
    "It focuses on the most common root causes:\n",
    "- wrong `predict_proba` column (classes order not `[0, 1]`)\n",
    "- wrong calibrator file loaded\n",
    "- feature mismatch / distribution shift\n",
    "- extreme disagreement between CNN and calibrator\n",
    "\n",
    "It will:\n",
    "1. Load calibrator and print `classes_`, coefficients, intercept.\n",
    "2. Define a safe `proba_fake()` helper (select by class label).\n",
    "3. Score a handful of files and compare **CNN-only** vs **calibrated**.\n",
    "4. Recompute calibrator probabilities two ways (`[:,1]` vs `proba_fake`) to detect column mistakes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Imports ----\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Your scorer module (must exist)\n",
    "from src.inference.scorer import AudioSpoofScorer, ScorerConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebfc5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Paths / config ----\n",
    "CNN_MODEL = \"models/cnn/audio_cnn_balanced_best.pt\"\n",
    "\n",
    "# Point this at the exact calibrator you think you are using\n",
    "CALIBRATOR_PATH = \"models/calibrators/agg_lr_real_or_fake_new.joblib\"\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# Dataset dirs (must have real/ and fake/ subfolders) for quick comparison\n",
    "REAL_OR_FAKE_DIR = \"data/audio/processed/real_or_fake\"\n",
    "ASVSPOOF_DIR = None  # optionally set e.g. \"data/audio/processed/asvspoof_2021_df\"\n",
    "\n",
    "# Sampling\n",
    "N_PER_CLASS = 50\n",
    "SEED = 42\n",
    "\n",
    "print(\"CNN_MODEL exists:\", Path(CNN_MODEL).exists())\n",
    "print(\"CALIBRATOR exists:\", Path(CALIBRATOR_PATH).exists())\n",
    "print(\"REAL_OR_FAKE_DIR exists:\", Path(REAL_OR_FAKE_DIR).exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b6f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load calibrator and inspect ----\n",
    "obj = joblib.load(CALIBRATOR_PATH)\n",
    "cal = obj[\"aggregator\"] if isinstance(obj, dict) and \"aggregator\" in obj else obj\n",
    "\n",
    "print(\"Calibrator type:\", type(cal))\n",
    "print(\"classes_:\", getattr(cal, \"classes_\", None))\n",
    "\n",
    "# Coefs / intercept (for LogisticRegression / linear models)\n",
    "if hasattr(cal, \"coef_\"):\n",
    "    print(\"coef_.shape:\", cal.coef_.shape)\n",
    "    print(\"intercept_:\", cal.intercept_)\n",
    "    if isinstance(obj, dict) and \"feature_names\" in obj:\n",
    "        print(\"feature_names:\", obj[\"feature_names\"])\n",
    "else:\n",
    "    print(\"No coef_ (might be a pipeline or wrapped estimator).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc41c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Safe probability extraction: pick the column for 'fake' by label ----\n",
    "def proba_fake(calibrator, X):\n",
    "    proba = calibrator.predict_proba(X)\n",
    "    classes = list(getattr(calibrator, \"classes_\", []))\n",
    "\n",
    "    # integer label case: fake==1\n",
    "    if 1 in classes:\n",
    "        idx = classes.index(1)\n",
    "        return proba[:, idx]\n",
    "\n",
    "    # string label cases\n",
    "    for fake_name in (\"fake\", \"spoof\", \"bonafide_fake\", \"1\"):\n",
    "        if fake_name in classes:\n",
    "            idx = classes.index(fake_name)\n",
    "            return proba[:, idx]\n",
    "\n",
    "    raise ValueError(f\"Cannot find fake class in calibrator.classes_: {classes}\")\n",
    "\n",
    "X_dummy = np.zeros((3, 5), dtype=np.float32)\n",
    "\n",
    "print(\"predict_proba shape:\", cal.predict_proba(X_dummy).shape)\n",
    "print(\"predict_proba[:,1] sample:\", cal.predict_proba(X_dummy)[:, 1])\n",
    "\n",
    "try:\n",
    "    print(\"proba_fake() sample:\", proba_fake(cal, X_dummy))\n",
    "except Exception as e:\n",
    "    print(\"proba_fake() failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2908bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Instantiate scorer (calibrator ON) ----\n",
    "scorer = AudioSpoofScorer(\n",
    "    cnn_model_path=CNN_MODEL,\n",
    "    calibrator_path=CALIBRATOR_PATH,\n",
    "    device=DEVICE,\n",
    "    config=ScorerConfig(n_segments=6, clip_seconds=4.0, cnn_agg=\"median\", low_thr=0.3, high_thr=0.7),\n",
    ")\n",
    "print(\"Scorer calibrator loaded:\", scorer.calibrator is not None)\n",
    "print(\"Scorer calibrator classes_:\", getattr(scorer.calibrator, \"classes_\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper: list & sample files ----\n",
    "import random\n",
    "\n",
    "AUDIO_EXTS = {\".wav\", \".flac\", \".mp3\", \".ogg\", \".m4a\"}\n",
    "\n",
    "def list_audio_files(root: Path):\n",
    "    return sorted([p for p in root.rglob(\"*\") if p.is_file() and p.suffix.lower() in AUDIO_EXTS])\n",
    "\n",
    "def sample_files(base_dir: str, n_per_class: int = 50, seed: int = 42):\n",
    "    base = Path(base_dir)\n",
    "    real = list_audio_files(base / \"real\")\n",
    "    fake = list_audio_files(base / \"fake\")\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    if n_per_class is not None:\n",
    "        if len(real) > n_per_class:\n",
    "            real = rng.sample(real, n_per_class)\n",
    "        if len(fake) > n_per_class:\n",
    "            fake = rng.sample(fake, n_per_class)\n",
    "\n",
    "    files = [(p, 0) for p in sorted(real)] + [(p, 1) for p in sorted(fake)]\n",
    "    return files\n",
    "\n",
    "files_rof = sample_files(REAL_OR_FAKE_DIR, n_per_class=N_PER_CLASS, seed=SEED)\n",
    "print(\"Sampled files:\", len(files_rof), \"(real+fake)\")\n",
    "print(\"Example:\", files_rof[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e12a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Score files and compare CNN vs calibrator (API output) ----\n",
    "rows = []\n",
    "for p, y in files_rof:\n",
    "    rep = scorer.score_file(str(p), threshold=None)  # no decision needed\n",
    "    if not rep.get(\"ok\", False):\n",
    "        rows.append({\"path\": str(p), \"y_true\": y, \"ok\": False, \"error\": rep.get(\"error\")})\n",
    "        continue\n",
    "\n",
    "    sig = rep.get(\"signals\") or {}\n",
    "    meta = rep.get(\"meta\") or {}\n",
    "\n",
    "    rows.append({\n",
    "        \"path\": str(p),\n",
    "        \"y_true\": y,\n",
    "        \"ok\": True,\n",
    "        \"cnn_only_score\": float(sig.get(\"cnn_only_score\")),\n",
    "        \"calibrated_score_api\": float(sig.get(\"calibrated_score\")) if sig.get(\"calibrated_score\") is not None else np.nan,\n",
    "        \"cnn_median\": float(sig.get(\"cnn_median\")),\n",
    "        \"cnn_max\": float(sig.get(\"cnn_max\")),\n",
    "        \"cnn_var\": float(sig.get(\"cnn_var\")),\n",
    "        \"total_seconds\": float(sig.get(\"total_seconds\")),\n",
    "        \"silence_ratio\": float(sig.get(\"silence_ratio\")),\n",
    "        \"effective_clip_seconds\": meta.get(\"effective_clip_seconds\"),\n",
    "        \"effective_n_segments\": meta.get(\"effective_n_segments\"),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df_ok = df[df[\"ok\"]].copy()\n",
    "df_ok.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Recompute calibrator probs locally using BOTH methods ----\n",
    "X = df_ok[[\"cnn_median\", \"cnn_max\", \"cnn_var\", \"total_seconds\", \"silence_ratio\"]].to_numpy(dtype=np.float32)\n",
    "\n",
    "p_col1 = cal.predict_proba(X)[:, 1]\n",
    "p_safe = proba_fake(cal, X)\n",
    "\n",
    "df_ok[\"cal_proba_col1\"] = p_col1.astype(float)\n",
    "df_ok[\"cal_proba_safe\"] = p_safe.astype(float)\n",
    "\n",
    "df_ok[[\"cnn_only_score\", \"calibrated_score_api\", \"cal_proba_col1\", \"cal_proba_safe\"]].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac20e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Which method matches the scorer output? ----\n",
    "diff_col1 = float(np.nanmean(np.abs(df_ok[\"calibrated_score_api\"] - df_ok[\"cal_proba_col1\"])))\n",
    "diff_safe = float(np.nanmean(np.abs(df_ok[\"calibrated_score_api\"] - df_ok[\"cal_proba_safe\"])))\n",
    "\n",
    "print(\"Mean abs diff vs scorer - [:,1]:\", diff_col1)\n",
    "print(\"Mean abs diff vs scorer - proba_fake:\", diff_safe)\n",
    "\n",
    "if diff_safe < diff_col1:\n",
    "    print(\"✅ Scorer output matches class-aware probability (or safe column).\")\n",
    "else:\n",
    "    print(\"⚠️ Scorer output matches [:,1]. If classes_ isn't [0,1], this can be wrong.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40420a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Disagreement analysis ----\n",
    "df_ok[\"delta\"] = (df_ok[\"cnn_only_score\"] - df_ok[\"calibrated_score_api\"]).abs()\n",
    "print(df_ok[\"delta\"].describe())\n",
    "\n",
    "# Show the biggest CNN vs calibrator disagreements\n",
    "df_ok.sort_values(\"delta\", ascending=False).head(25)[\n",
    "    [\"path\", \"y_true\", \"cnn_only_score\", \"calibrated_score_api\", \"delta\", \"total_seconds\", \"silence_ratio\", \"cnn_var\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f8e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualize score distributions ----\n",
    "plt.figure()\n",
    "plt.hist(df_ok[df_ok[\"y_true\"]==0][\"cnn_only_score\"].values, bins=30, alpha=0.7, label=\"real (cnn)\")\n",
    "plt.hist(df_ok[df_ok[\"y_true\"]==1][\"cnn_only_score\"].values, bins=30, alpha=0.7, label=\"fake (cnn)\")\n",
    "plt.title(\"CNN-only score distribution (real_or_fake)\")\n",
    "plt.xlabel(\"cnn_only_score\"); plt.ylabel(\"count\"); plt.legend(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(df_ok[df_ok[\"y_true\"]==0][\"calibrated_score_api\"].values, bins=30, alpha=0.7, label=\"real (cal)\")\n",
    "plt.hist(df_ok[df_ok[\"y_true\"]==1][\"calibrated_score_api\"].values, bins=30, alpha=0.7, label=\"fake (cal)\")\n",
    "plt.title(\"Calibrated score distribution (scorer output) (real_or_fake)\")\n",
    "plt.xlabel(\"calibrated_score_api\"); plt.ylabel(\"count\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12886e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Optional: run the same comparison for ASVSPOOF_DIR ----\n",
    "if ASVSPOOF_DIR:\n",
    "    files_asv = sample_files(ASVSPOOF_DIR, n_per_class=N_PER_CLASS, seed=SEED)\n",
    "    rows2 = []\n",
    "    for p, y in files_asv:\n",
    "        rep = scorer.score_file(str(p), threshold=None)\n",
    "        if not rep.get(\"ok\", False):\n",
    "            continue\n",
    "        sig = rep.get(\"signals\") or {}\n",
    "        rows2.append({\n",
    "            \"path\": str(p),\n",
    "            \"y_true\": y,\n",
    "            \"cnn_only_score\": float(sig.get(\"cnn_only_score\")),\n",
    "            \"calibrated_score_api\": float(sig.get(\"calibrated_score\")) if sig.get(\"calibrated_score\") is not None else np.nan,\n",
    "            \"total_seconds\": float(sig.get(\"total_seconds\")),\n",
    "            \"silence_ratio\": float(sig.get(\"silence_ratio\")),\n",
    "            \"cnn_var\": float(sig.get(\"cnn_var\")),\n",
    "        })\n",
    "    df_asv = pd.DataFrame(rows2)\n",
    "    print(\"ASV rows:\", len(df_asv))\n",
    "    display(df_asv.head())\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(df_asv[df_asv[\"y_true\"]==0][\"cnn_only_score\"].values, bins=30, alpha=0.7, label=\"real (cnn)\")\n",
    "    plt.hist(df_asv[df_asv[\"y_true\"]==1][\"cnn_only_score\"].values, bins=30, alpha=0.7, label=\"fake (cnn)\")\n",
    "    plt.title(\"CNN-only score distribution (ASV)\")\n",
    "    plt.legend(); plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(df_asv[df_asv[\"y_true\"]==0][\"calibrated_score_api\"].values, bins=30, alpha=0.7, label=\"real (cal)\")\n",
    "    plt.hist(df_asv[df_asv[\"y_true\"]==1][\"calibrated_score_api\"].values, bins=30, alpha=0.7, label=\"fake (cal)\")\n",
    "    plt.title(\"Calibrated score distribution (ASV)\")\n",
    "    plt.legend(); plt.show()\n",
    "else:\n",
    "    print(\"ASVSPOOF_DIR is None. Set it above if you want to test ASV dataset too.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da5d9f2",
   "metadata": {},
   "source": [
    "## What to do with the results\n",
    "\n",
    "- If `classes_` is not `[0, 1]` (with `1` meaning fake), and the scorer uses `[:, 1]`, fix the scorer to select by class label.\n",
    "- If the calibrator distribution is clearly wrong for short clips, gate calibrator usage or refit it using features computed with your current scoring policy.\n",
    "- If CNN-only separates but calibrated does not, the calibrator is the issue (not the CNN).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
