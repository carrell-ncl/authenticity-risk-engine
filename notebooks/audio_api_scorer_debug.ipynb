{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4887ef",
   "metadata": {},
   "source": [
    "# Audio Authenticity API / Scorer Quick Test Notebook\n",
    "\n",
    "This notebook helps you sanity-check your **local scorer (the same code used by the FastAPI)** against:\n",
    "\n",
    "- a single file\n",
    "- a directory structured as:\n",
    "  - `.../real/`\n",
    "  - `.../fake/`\n",
    "\n",
    "It will print example outputs, compute confusion matrices at a threshold, and show basic score distributions.\n",
    "\n",
    "> Assumes you have `src/inference/scorer.py` with `AudioSpoofScorer` and your CNN checkpoint + (optional) calibrator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Imports ----\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "# Import your scorer (same one used by the API)\n",
    "from src.inference.scorer import AudioSpoofScorer, ScorerConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab6ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Configure paths ----\n",
    "CNN_MODEL = \"models/cnn/audio_cnn_balanced_best.pt\"\n",
    "CALIBRATOR = \"models/calibrators/agg_lr_real_or_fake_new.joblib\"  # set to None to disable\n",
    "DEVICE = \"cpu\"  # or \"cuda\"\n",
    "\n",
    "# Dataset directories (must have real/ and fake/ subfolders)\n",
    "REAL_OR_FAKE_DIR = \"data/audio/processed/real_or_fake\"\n",
    "ASVSPOOF21_DIR = None  # e.g. \"data/audio/asvspoof21_eval\" (must have real/ fake/ if you want directory eval)\n",
    "\n",
    "# Single-file test (optional)\n",
    "TEST_FILE = \"data/audio/processed/real_or_fake/real/file20.wav_16k.wav_norm.wav_mono.wav_silence.wav\"\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "print('CNN_MODEL exists:', Path(CNN_MODEL).exists())\n",
    "print('CALIBRATOR exists:', Path(CALIBRATOR).exists() if CALIBRATOR else None)\n",
    "print('REAL_OR_FAKE_DIR exists:', Path(REAL_OR_FAKE_DIR).exists())\n",
    "print('TEST_FILE exists:', Path(TEST_FILE).exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Instantiate scorer ----\n",
    "scorer = AudioSpoofScorer(\n",
    "    cnn_model_path=CNN_MODEL,\n",
    "    calibrator_path=CALIBRATOR if CALIBRATOR and Path(CALIBRATOR).exists() else None,\n",
    "    device=DEVICE,\n",
    "    config=ScorerConfig(n_segments=6, clip_seconds=4.0, cnn_agg=\"median\", low_thr=0.3, high_thr=0.7),\n",
    ")\n",
    "print(\"Loaded scorer. Calibrator enabled:\", scorer.calibrator is not None)\n",
    "print(\"CNN cfg sample_rate:\", scorer.cfg.get(\"sample_rate\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0635e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Score a single file ----\n",
    "report = scorer.score_file(TEST_FILE, threshold=THRESHOLD)\n",
    "report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d508f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper: score a dataset directory ----\n",
    "AUDIO_EXTS = {\".wav\", \".flac\", \".mp3\", \".ogg\", \".m4a\"}  # torchaudio support varies by build\n",
    "\n",
    "def list_audio_files(root: Path):\n",
    "    return sorted([p for p in root.rglob(\"*\") if p.is_file() and p.suffix.lower() in AUDIO_EXTS])\n",
    "\n",
    "def score_dataset_dir(base_dir: str, threshold: float = 0.5):\n",
    "    base = Path(base_dir)\n",
    "    real_dir = base / \"real\"\n",
    "    fake_dir = base / \"fake\"\n",
    "    assert real_dir.is_dir() and fake_dir.is_dir(), f\"Expected {real_dir} and {fake_dir}\"\n",
    "\n",
    "    rows = []\n",
    "    for label_name, y_true in [(\"real\", 0), (\"fake\", 1)]:\n",
    "        files = list_audio_files(base / label_name)\n",
    "        for f in files:\n",
    "            rep = scorer.score_file(str(f), threshold=threshold)\n",
    "            if not rep.get(\"ok\", False):\n",
    "                rows.append({\n",
    "                    \"path\": str(f),\n",
    "                    \"y_true\": y_true,\n",
    "                    \"ok\": False,\n",
    "                    \"error\": rep.get(\"error\"),\n",
    "                })\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"path\": str(f),\n",
    "                \"y_true\": y_true,\n",
    "                \"ok\": True,\n",
    "                \"score\": rep[\"score\"],\n",
    "                \"tier\": rep[\"tier\"],\n",
    "                \"decision\": rep[\"decision\"],\n",
    "                \"cnn_only_score\": rep[\"signals\"][\"cnn_only_score\"],\n",
    "                \"calibrated_score\": rep[\"signals\"][\"calibrated_score\"],\n",
    "                \"cnn_median\": rep[\"signals\"][\"cnn_median\"],\n",
    "                \"cnn_max\": rep[\"signals\"][\"cnn_max\"],\n",
    "                \"cnn_var\": rep[\"signals\"][\"cnn_var\"],\n",
    "                \"silence_ratio\": rep[\"signals\"][\"silence_ratio\"],\n",
    "                \"total_seconds\": rep[\"signals\"][\"total_seconds\"],\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "def summarize_df(df: pd.DataFrame, score_col: str = \"score\", threshold: float = 0.5):\n",
    "    d = df[df[\"ok\"]].copy()\n",
    "    y_true = d[\"y_true\"].to_numpy()\n",
    "    probs = d[score_col].to_numpy(dtype=float)\n",
    "    y_pred = (probs >= threshold).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    auc = roc_auc_score(y_true, probs) if len(np.unique(y_true)) > 1 else np.nan\n",
    "\n",
    "    real_acc = (y_pred[y_true==0] == 0).mean() if (y_true==0).any() else np.nan\n",
    "    fake_acc = (y_pred[y_true==1] == 1).mean() if (y_true==1).any() else np.nan\n",
    "    overall_acc = (y_pred == y_true).mean() if len(y_true) else np.nan\n",
    "\n",
    "    tp = int(((y_pred==1) & (y_true==1)).sum())\n",
    "    fp = int(((y_pred==1) & (y_true==0)).sum())\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "\n",
    "    return {\n",
    "        \"n_ok\": int(len(d)),\n",
    "        \"auc\": float(auc) if not np.isnan(auc) else None,\n",
    "        \"real_accuracy\": float(real_acc) if not np.isnan(real_acc) else None,\n",
    "        \"fake_accuracy\": float(fake_acc) if not np.isnan(fake_acc) else None,\n",
    "        \"overall_accuracy\": float(overall_acc) if not np.isnan(overall_acc) else None,\n",
    "        \"precision\": float(precision),\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"labels\": [\"real\",\"fake\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcaebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Score REAL_OR_FAKE dataset ----\n",
    "df_rof = score_dataset_dir(REAL_OR_FAKE_DIR, threshold=THRESHOLD)\n",
    "df_rof.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0482f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Summary (final score) ----\n",
    "summary_final = summarize_df(df_rof, score_col=\"score\", threshold=THRESHOLD)\n",
    "summary_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Summary (CNN-only score) ----\n",
    "summary_cnn = summarize_df(df_rof, score_col=\"cnn_only_score\", threshold=THRESHOLD)\n",
    "summary_cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecffc6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Investigate constant segment scores (like your API output) ----\n",
    "# If segments are identical for many files, this will show it quickly.\n",
    "# (Often indicates audio shorter than clip_seconds and being heavily padded, or model saturation.)\n",
    "d = df_rof[df_rof[\"ok\"]].copy()\n",
    "print(\"Total ok:\", len(d))\n",
    "print(\"Duration seconds - min/median/max:\", d[\"total_seconds\"].min(), d[\"total_seconds\"].median(), d[\"total_seconds\"].max())\n",
    "print(\"cnn_var - min/median/max:\", d[\"cnn_var\"].min(), d[\"cnn_var\"].median(), d[\"cnn_var\"].max())\n",
    "\n",
    "# How many have (near) zero variance?\n",
    "print(\"Fraction with cnn_var < 1e-6:\", float((d[\"cnn_var\"] < 1e-6).mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea697fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Score distributions (histograms) ----\n",
    "d = df_rof[df_rof[\"ok\"]].copy()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(d[d[\"y_true\"]==0][\"score\"].values, bins=30, alpha=0.7, label=\"real\")\n",
    "plt.hist(d[d[\"y_true\"]==1][\"score\"].values, bins=30, alpha=0.7, label=\"fake\")\n",
    "plt.title(\"Score distribution (final score)\")\n",
    "plt.xlabel(\"score\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(d[d[\"y_true\"]==0][\"cnn_only_score\"].values, bins=30, alpha=0.7, label=\"real\")\n",
    "plt.hist(d[d[\"y_true\"]==1][\"cnn_only_score\"].values, bins=30, alpha=0.7, label=\"fake\")\n",
    "plt.title(\"Score distribution (CNN-only)\")\n",
    "plt.xlabel(\"cnn_only_score\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c6c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Optional: score ASVspoof directory (if structured as real/ fake/) ----\n",
    "if ASVSPOOF21_DIR:\n",
    "    df_asv = score_dataset_dir(ASVSPOOF21_DIR, threshold=THRESHOLD)\n",
    "    print(\"ASV ok:\", df_asv[\"ok\"].sum(), \"of\", len(df_asv))\n",
    "    print(\"Final score summary:\", summarize_df(df_asv, score_col=\"score\", threshold=THRESHOLD))\n",
    "    print(\"CNN-only summary:\", summarize_df(df_asv, score_col=\"cnn_only_score\", threshold=THRESHOLD))\n",
    "else:\n",
    "    print(\"ASVSPOOF21_DIR is None. Set it above to run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6155a2fa",
   "metadata": {},
   "source": [
    "## Notes / Debug Tips\n",
    "\n",
    "If you see patterns like:\n",
    "- **all segments identical** (cnn_var ≈ 0)\n",
    "- **everything predicted fake** or **everything predicted real**\n",
    "\n",
    "Common causes:\n",
    "- many files shorter than `clip_seconds` → heavy padding + identical segment windows\n",
    "- mismatch between training and inference transforms (cfg must match checkpoint)\n",
    "- label direction inversion (interpreting `sigmoid(logit)` as P(fake) when it is P(real))\n",
    "- calibrator applied out-of-domain (try disabling calibrator and re-check)\n",
    "\n",
    "This notebook prints duration statistics and shows distributions so you can spot these quickly.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
